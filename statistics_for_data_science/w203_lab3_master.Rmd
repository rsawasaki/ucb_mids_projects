---
title: '__W203: Statistics for Data Science__'
author:
- Robert Louka
- Ryan Sawasaki
- Joshua Noble
- Praveen Joseph
subtitle: '__LAB 3: Reducing Crime__'
output:
  html_document:
    df_print: paged
    self_contained: no
  word_document: default
  pdf_document: default
---


```{r echo=FALSE, warning=FALSE, message=FALSE}
library(car)
library(lmtest)
library(sandwich)
library(stargazer)
library(ggplot2)
library(tidyr)
library(knitr)
library(dplyr)
library(reshape2)
library(corrplot)


opts_chunk$set(tidy.opts=list(width.cutoff=20),tidy=TRUE)

library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})


```

```{r echo=FALSE}
#read in the data

data <- read.csv(file= "C:\\Users\\winbase\\MIDS\\w203\\w203_lab3\\data\\crime_v2.csv") #josh
#data <- read.csv(file = 'H:/ROL/MIDS/W203 Stats/lab_3/crime_v2.csv') #Robert
#data <- read.csv(file = '~/Desktop/W203/w203_lab3-master/crime_v2.csv') #Praveen


```

# 1. An Introduction

As crime has seen an increase in the 1980’s, citizens of North Carolina have been looking to local government politicians to address this growing problem. In preparation for the upcoming election, our team of political consultants has been tasked with providing insight to drive policy directed at reducing crime levels. Before pushing a political campaign aimed at crime reduction, we must first identify the key determinants of crime and their significance in order to properly focus resources to target these issues.

Many studies have examined numerous potential determinants of crimes and it remains a complex and evolving issue. Traditionally, criminal activity is often linked to issues of inequality and poverty. In addition, factors revolving around the criminal justice system are often viewed as having a significant impact, both positive and negative, on crime rate. While there is little debate that these variables affect crime, a one size fits all policy on crime does not properly address the unique issues at the state and county levels. This report aims to identify the complex interactions of crime determinants in North Carolina using recently compiled statistics from FBI and government agencies. 

While many studies have been conducted on individual crime factors, this report examines multiple factors holistically. The primary research question this report addresses is: Which demographic, economic and deterrent factors significantly affect crime? To answer this question, our team has been provided a dataset of 1987 statistics from select North Carolina counties. There are 100 counties in the state, however, 10 counties have been omitted from the dataset. Given that the omitted counties comprise of a small percentage of the total population of the state (less than 2%), the results of this study are not significantly impacted by the omission of the 10 counties. The data provided for this study has been pulled from multiple credible sources including:\
* FBI’s Uniform Crime Reports\
* FBI’s police agency employee counts\
* North Carolina Department of Correction\
* North Carolina Employment Security Commission\
* Census Data\

Our dependent variable and the key measure we are focused on is crime rate, which is defined as crimes committed per person. Our independent variables have been grouped into categories of deterrent, demographic, economic, and geographical factors. A comprehensive list of the variables and their respective categories are described in our exploratory data analysis.

While this 1987 dataset provides observational variables that impact crime, the dataset does not provide a comprehensive list of all variables. There are a number of factors that our team has identified that could potentially assist in more accurately measuring a causal effect on crime. These factors are discussed in further detail in the omitted variables section of this report. 

The provided dataset only covers a single cross-section of the data from the year 1987. Our team conducted additional outside research to uncover a multi-year panel of corresponding crime data to assist in cross-checking the accuracy of the wage data with the provided dataset. However, the statistical results of this report are limited to the provided 1987 data. A major issue with only using a single cross-section of data is that lag effects cannot be observed. This pertains to the police presence variable, which it is expected that the effects of an increase or decrease in police force may lag for years. In addition, lag effects may also be responsible for the probability of conviction (ratio of convictions to arrests) variable being greater than 1 in some counties. It was surmised that a probability greater than 1 was due to mulitple convictions and convictions potentially occuring years after the arrest. So in a given year, there may be more convictions than arrests.  

Using this 1987 dataset, our team has conducted a study on the determinants of crime and prepared recommendations for a political strategy addressing this issue in North Carolina.  
\pagebreak


# 2. A Model Building Process

## Exploratory Data Analaysis

We started by conducting exploratory data analysis. First, we read the original paper [CORNWELL – TRUMBULL (1994)] to get a better understanding of each variable. We defined the variables in the table below and grouped them into five groups (the group construction is discussed in the groupings section below). 

```{r echo=FALSE}
crime_count <- c(1:25)
data_variables <- c("county","year","crmrte","prbarr","prbconv","prbpris","avgsen","polpc","density","taxpc","west","central","urban","pctmin80","wcon","wtuc","wtrd","wfir","wser","wmfg","wfed","wsta","wloc","mix","pctymle")
data_description <- c("county identifier","1987","crimes committed per person","'probability' of arrest","'probability' of conviction","'probability' of prison sentence","avg. sentence, days","police per capita","people per sq. mile","tax revenue per capita","=1 if in western N.C.","=1 if in central N.C.","=1 if in SMSA","perc. minority, 1980","weekly wage, construction","wkly wge, trns, util, commun","wkly wge, whlesle, retail trade","wkly wge, fin, ins, real est","wkly wge, service industry","wkly wge, manufacturing","wkly wge, fed employees","wkly wge, state employees","wkly wge, local gov emps","offense mix: face-to-face/other","percent young male")
data_group <- c("Control","","","Deterrent","Deterrent","Deterrent","Deterrent","Deterrent","Demographic","Demographic","Region","Region","Urban","Demographic","Wages","Wages","Wages","Wages","Wages","Wages","Wages","Wages","Wages","Demographic","Demographic")
data_notes <- c("","","ratio of FBI index crimes to county population","ratio of arrests to offenses","ratio of convictions to arrests","proportion of total convictions resulting in prison sentences","average sentence in days","","country population divided by county land area","","dummy","dummy","dummy","proportion of country population that is minority or nonwhite","average weekly wage in that sector","average weekly wage in that sector","average weekly wage in that sector","average weekly wage in that sector","average weekly wage in that sector","average weekly wage in that sector","average weekly wage in that sector","average weekly wage in that sector","average weekly wage in that sector","ratio of face-to-face crimes (robbery, assault, rape) to non-face-to-face crimes","proportion of country population that is male between 15 and 24"
)
data_headers <- c("Variable", "Description", "Group", "Note")
data_table <- data.frame(data_variables, data_description, data_group, data_notes)
kable(data_table, col.names = data_headers, caption = "Descriptions and Groups of Variables")

```

To get a broad sense of the data set the summary function was run.

```{r results=FALSE}
summary(data)
```

This function provides a high level view of each variable. Six rows have missing values for all variables. In addition, there is one duplicate row. Also the variable prbconv is loaded as a factor, so it needs to be converted to numeric. These issues are handled below to create the initial data set. 

```{r}
#eliminate N/A's (6 rows of NA were removed)
data_crmrte <- data[!is.na(data$crmrte),]

#remove duplicates (1 duplicate record was found)
data_crmrte <- data_crmrte %>% distinct()

#prbconv was defined as factor , we will convert it to numeric
data_crmrte$prbconv <- as.numeric(as.character(data_crmrte$prbconv))
#class(data_crmrte$prbconv)
```

With 25 original variables in the data set the natural place to start is with the dependent variable, crmrte. To get a better sense of this variable, the distribution is graphed below.

```{r out.width="50%", fig.show='hold'}
quantile(data_crmrte$crmrte, c(0, .01, .05, .10, .25, .50,  .75, .90, .95, .99, 1.0))
hist(data_crmrte$crmrte,breaks=20, xlab="Crime Rate", main="Histogram of Crime Rate")
boxplot(data_crmrte$crmrte, main="Boxplot of Crime Rate")
# The box plot and histogram show signs of right skew in the crime rate 
```

```{r out.width="50%", fig.show='hold'}
plot(data_crmrte$crmrte)
qqnorm(data_crmrte$crmrte) # The q-q plot shows sign of non-normality in crime rate
shapiro.test(data_crmrte$crmrte) # Shapiro-wilk test confirms the non-normality
```
## Outlier Analysis

\
There are several outliers in the variable crmrte and the distribution is right skewed. With our sample size non-normality is not a top concern but this distribution is not perfectly normal. We analyze outliers for crime rate that are > 2*Std-dev from the mean crime rate (i.e data pts with crime rate > 0.07)

The postiviely skewed outliers (6 counties) on the right side of the distribution are examined to gather some insights:

1. 4 of out of the 6 outliers are in urban areas
2. The average demographic density for the outlier set is  greater than 3 times the average density for the overall sample
3. We also observe that data ppt 53 (county 119) which has the highest crime rate, also has the highest density amongst the outliers and is a urban area

This is not very surprising as we expect urban areas with high density of population to have more crimes. we will continue to monitor the impact of the outliers and conisder the treatment of these outlier in a later part of the report.

```{r tidy=TRUE}
upper <- data_crmrte[data_crmrte$crmrte > 0.07,]
density_table <- data.frame(upper$county, upper$crmrte, upper$density)
kable(round(density_table,2), col.names = c("County", "Crime Rate ", " Density"), 
      caption = "Density and Outliers")
```


We also look at the lower range of outliers and find only observation 51 (county 115) which has crime rate < 0.01. This outlier data pt (county 115) has some significant outlier characterestics. County 115  has the lowest crime rate in the data set, extremely low density, the highest polpc (police per capita), the highest avg sentence, the third highest prbconv, and the lowest pctmin80. 

One possible explanation is that county 115 could be a army/marine base or high security government facility (such as FBI field office, NSA or CIA facility)  with national security concerns which explains the highest polpc. This also might explain the very high avgsen ( 20+ yrs) for crimes convicted in this country tend to have a higher severity/penalty. If this were the case, it would stand to reason that there are very civilan inhabitants and also explain why the population density is very low and crime rate is the lowest with probability of arrest and conviction both >1. 

We can argue that this county is certainly an outlier, but not one that should be removed from the data as it represents a plausible county's observation in the dataset.
\



```{r}
lower <- data_crmrte[data_crmrte$crmrte < 0.01,]
density_table <- data.frame(lower$county, lower$crmrte, lower$density, lower$polpc, lower$avgsen, lower$prbarr, lower$prbconv )
kable(round(density_table,4), col.names = c("County", "Crime Rate ", " Density", "Police", "Avg Sentence", "Prob of Arrest", "Prob of Conv"), 
      caption = "Special Outliers")

```

For campaign purposes, we want to predict crime. We want our candidate to be able to say that he or she can reduce crime in order to win votes. What is the most effective way to convey that? Using crime rate as it appears in the data set is using the level of crime rate and would suggest the following statement as a campaign slogan - "I can reduce crime to this rate by doing x, y, and z". 

Transforming crime rate into the log of crime rate allows for the statement "I can reduce crime by n% by doing x, y, and z." We find the latter more powerful and meaningful to voters since most voters have no idea about the level of crime rates. In addition, we will show that the transformation of crime rate improves the normality and distribution of the variable, which will often reduce skew in the errors as well.\
```{r out.width="50%", fig.show='hold'}
data_crmrte$log_crmrte <- log(data_crmrte$crmrte)
hist(data_crmrte$log_crmrte,breaks=20, xlab="Log Crime Rate", main="Histogram of Log Crime Rate")
boxplot(data_crmrte$log_crmrte, main="Boxplot of log of crmrte")
```

```{r out.width="50%", fig.show='hold'}
plot(data_crmrte$log_crmrte, ylab = "Log Crime Rate")
qqnorm(data_crmrte$log_crmrte)
shapiro.test(data_crmrte$log_crmrte) # Shapiro-wilk test confirms the log transformation was able to eradicate the non-normality in the dependent variable

```
The histogram of the transformed crime rate is much more symmetrical and shows much less right skew. The box plot shows all of the outliers on the high end have been removed, though an outlier (county 115) on the low end has been become more prominent. 

The scatter plot looks much more normal, and the Q-Q plot is much closer to normal with the data points hugging the 45 degree line much more closely. Given the stronger argument for the political campaign and the benefits to normality we have chosen to model the tranformation of crime rate as opposed to crime rate.

The Shapiro-wilk test fails to reject the Null Hypothesis of Normality, thereby confirming that the log transformation was able establish normal distribution of the data.
\

# Panel Data and Further Data Vetting 

We also were able to find the original panel data set. While we won't use this for prediction, we will use it for data vetting. We created three data sets. The first data set is comprised of the entire panel data which includes data from 1981-1987. The second data set is the data from the first 6 years and excludes the data set given in the assignment. Finally, the third data set is just the year 1987 and the columns in our assignment. 

```{r}
#load the new dataset
data_panel <- read.table(file = 'C:/Users/winbase/MIDS/w203/w203_lab3/crime4.txt', sep="", header=FALSE)
#name the columns of the new dataset -->
colnames(data_panel) <- c("county", "year", "crmrte", "prbarr", "prbconv", "prbpris", "avgsen", "polpc","density", "taxpc","west", "central", "urban", "pctmin80", "wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", "wsta","wloc", "mix", "pctymle", "d82", "d83", "d84", "d85", "d86", "d87", "lcrmrte", "lprbarr", "lprbconv","lprbpris", "lavgsen","lpolpc", "ldensity", "ltaxpc", "lwcon", "lwtuc", "lwtrd", "lwfir", "lwser", "lwmfg","lwfed", "lwsta", "lwloc", "lmix", "lpctymle", "lpctmin", "clcrmrte", "clprbarr", "clprbcon", "clprbpri","clavgsen", "clpolpc", "cltaxpc", "clmix") 
data_panel$prbconv <- as.numeric(as.character(data_panel$prbconv)) 
#get all years besides 1987 for comparison 
data_panel_additional <- data_panel[data_panel$year != 87,] 
#get just the year 1987 for comparison 
data_panel_87 <- data_panel[data_panel$year == 87,c("county", "year", "crmrte", "prbarr", "prbconv", "prbpris","avgsen", "polpc","density", "taxpc","west", "central", "urban", "pctmin80", "wcon", "wtuc", "wtrd", "wfir","wser", "wmfg", "wfed","wsta","wloc", "mix", "pctymle")] 


#compare the assignment data set to the new data set in 1987
data_check <- data_crmrte[, !(colnames(data_crmrte)=="log_crmrte")] 
for (col in 1:ncol(data_check)) {
   for (row in 1:nrow(data_check)) {
       val_orig <- data_check[row, col]
       val_new  <- data_panel_87[row, col]
       if(abs(val_orig-val_new)>.001){ 
         print(paste("county=",data_check[row, 1], "year=",data_panel_87[row, 2], 
                     "column=",colnames(data_check)[col],"original=", data_check[row, col], 
                     "panel=",data_panel_87[row, col]))

         }
       }
 }

```


The code above compares the data set given to us to the same data extracted from the panel data. These data sets should match exactly. The print statements above identify two differences. For county 173 the panel data has a rounded value for density of .20342 while the given data set has a rounded value of .00002. Looking at the distribution for all years besides 1987, the minimum density for any county is .1977186 so the value in the given data set seems to be an extreme outlier, and it is much more likely that the value from the panel data set is correct. This value will be corrected below, and the quantiles are shown for evidence. In addition, county 71 is labeled as west in the given data set but for all years in the panel data set it has a zero value for west. This will also be corrected. 

```{r}
#Correct mistakes in original (assigned) data set
data_crmrte[data_crmrte$county==173, "density"] <- .2034221
data_crmrte[data_crmrte$county==71, "west"] <- 0

```


# Groupings

Having no background on this paper or in criminal justice in general we looked for ways to make this data more digestable. We decided to group the variables into categories to make it more manageable, and in this process we found five groups that seemed natural: deterrent, wages, demographic, region, and urban. We performed exploratory data analysis on all of these variables.\

The first group is deterrent data. As cited in the original paper, these variables were hypothesized to reduce crime rate through disincentivizing crime. Essentially, as the probability of getting caught increases, criminals' desire to commit crimes decreases.\

# Deterrent Data
```{r echo=FALSE, out.width="50%", fig.show='hold'}
deterrent_data <- data_crmrte[,c('prbarr','prbconv','prbpris','crmrte',
                          'avgsen','polpc')]

ggplot(gather(deterrent_data[,c('prbarr','prbconv','prbpris',
                                    'avgsen','polpc')]), aes(value)) + 
geom_histogram(bins = 10) + facet_wrap(~key, scales = 'free_x')

my_vars1 <- c("prbarr","prbconv","prbpris")
deterrent_data2 <- deterrent_data[my_vars1]
my_vars2 <- c("polpc")
deterrent_data3 <- deterrent_data[my_vars2]

boxplot(deterrent_data2, main="Boxplot of prbarr, prbconv, prbpris")

```
\
The first four histograms show right skew while prbpris shows left skew. Given their distributions, these variables are candidates to be transformed. In addition, these variables are more easily changed by a politican, which lends itself to a percent change argument similar to crime rate.\

# Wages Data

```{r fig.align='center'}
#create a dataframe of just the wage variables
wages_data <- data_crmrte[,c('wcon','wtuc','wtrd','wfir', 'wser',
                      'wmfg','wfed', 'wsta', 'wloc')]

#plot histograms of just the wage variables
ggplot(gather(wages_data), aes(value)) + 
  geom_histogram(bins = 10) + 
  facet_wrap(~key, scales = 'free_x')
```

```{r out.width="50%", fig.show='hold'}
#generate boxplots of just the wage variables
boxplot(log(wages_data), ylab="log(wage)", xlab="variable")
#Show outliers for entire 7 years
#boxplot(data_panel$wser, ylab="Service Wages", id=data_panel$county)
Boxplot(data_panel$wser, ylab="Service Wages", id=list(labels=data_panel$county, n=2, location="avoid"))

```

There is an obvious outlier for wser in County 185, (data pt 84 . The mean services wage across all the counties is $275 ( with a std dev of  206) and 84 has wser of 2177 (~9sd from mean), which seems like a measurement or typographical error. The next highest average weekly wage in any sector is 646 versus the value of 2177. In addition, the above boxplot shows it is an outlier for any wser for all counties over the entire panel data set; indeed it is an outlier for all sectors. Therefore, this observation will be corrected. In looking at this county for all years, the average wage appears to be increasing. While it is somewhat of a guess, it appears an extra 7 has been inserted into the value, so it is removed below.

```{r}
data_crmrte[data_crmrte$county==185, "wser"] <- 217.068
```


# Region Data
```{r fig.align='center'}
#create a dataframe of just the wage variables
dummies_data <- data_crmrte[,c('west','central')]

#plot histograms of just the dummy variables
ggplot(gather(dummies_data), aes(value)) + 
  geom_histogram(bins = 2) + 
  facet_wrap(~key)

#just a quick check that there is no overlap
region_check <- data_crmrte[which(data_crmrte$west == 1 && data_crmrte$central == 1)]

summary(region_check)
```
The regions are broken up into central, west, and east. East is left out of the data set and it's effect as the final level of the indicator variable will move to the intercept.\


# Urban Data
```{r}
#plot histograms of just the wage variables
sum(data_crmrte$urban) # There are only 8 Urban areas out of 90 counties
```
Urban did not fit into a great grouping so we left this variable on its own. A histogram shows that the state has relatively few urban counties, something to keep in mind when analyzing other variables such as density.\

# Demographic Data
```{r fig.align="center"}
#create a dataframe of just the demographic variables
demographic_data <- data_crmrte[,c('density', 'taxpc', 'pctmin80',
                                   'mix', 'pctymle')]

#plot histograms of just the demographic variables
ggplot(gather(demographic_data), aes(value)) + 
  geom_histogram(bins = 10) + 
  facet_wrap(~key, scales = 'free_x')
```
Once again we see a lot of right skewed distributions in the histograms and in the box plots.\

After exploring all of the variables we decided to tranform the other variables that are potentially under a politican's control - the deterrent variables. This gives us our final data set and so we can start running regressions.\

```{r fig.align="center"}
data_crmrte$prbconv <- as.numeric(as.character(data_crmrte$prbconv))
data_crmrte$log_prbarr <- log(data_crmrte$prbarr*100)
data_crmrte$log_prbconv <- log(data_crmrte$prbconv*100)
data_crmrte$log_prbpris <- log(data_crmrte$prbpris*100)
data_crmrte$log_avgsen <- log(data_crmrte$avgsen)
data_crmrte$log_polpc <- log(data_crmrte$polpc)
data_crmrte$log_taxpc <- log(data_crmrte$taxpc)


#plot histograms of just the demographic variables
ggplot(gather(data_crmrte[,c('log_prbarr', 'log_prbconv', 'log_prbpris', 'log_avgsen', 'log_polpc', 'log_taxpc') ]), aes(value)) + 
  geom_histogram(bins = 10) + 
  facet_wrap(~key, scales = 'free_x')

```
\
Though the distribution of the variables still exhibits skew, the skew does seem to be reduced.

# Log Tranformed Dependent Variable Comparison

\
In order to settle on the final data set we compare an all-in log-log model with an all-in log-linear to see which dependent variables are more suitable. This will explicitly test which dependent variables we should use - the log tranformed variables or the original variables. \

```{r fig.align="center"}
###### Initial Models #####
all_in_model <- lm(crmrte ~ prbarr + prbconv + prbpris 
                   + avgsen + polpc + density
                   + taxpc + west + central + urban + pctmin80 + wcon
                   + wtuc + wtrd + wfir + wser + wmfg 
                   + wfed + wsta + wloc
                   + mix + pctymle,
                   data = data_crmrte)
se.all_in_model = sqrt(diag(vcovHC(all_in_model)))

coeftest(all_in_model, vcov = vcovHC) # HC White SE

# Model residuals look normally distributed
hist(all_in_model$residuals) 

# plot(all_in_model)
#The residuals vs Fitted plot for this model quickly tell us 
#that ZCM ( MLR 4) and Random Sampling (MLR2) is violated for this model.

all_in_model_log_level <- lm(log_crmrte ~ prbarr + prbconv + prbpris 
                             + avgsen + polpc + density
                             + taxpc + west + central + urban 
                             + pctmin80 + wcon
                             + wtuc + wtrd + wfir + wser + wmfg 
                             + wfed + wsta + wloc
                             + mix + pctymle,
                             data = data_crmrte)
se.all_in_model_log_level = sqrt(diag(vcovHC(all_in_model_log_level)))
coeftest(all_in_model_log_level, vcov = vcovHC)

# Model residuals look somwewhat normally distributed
hist(all_in_model_log_level$residuals) 

# plot(all_in_model_log_level)
#The residuals vs Fitted plot for this model tell us that 
#the log transformation has significantly reduced the skew in the residuals 
#and the clustering and exogenity violations have also been ameliorated.



all_in_model_log_log <- lm(log_crmrte ~ log_prbarr + log_prbconv 
                             + log_prbpris + log_avgsen + log_polpc
                             + density+ log_taxpc + west + central 
                             + urban + pctmin80 + wcon
                             + wtuc + wtrd + wfir 
                             + wser + wmfg + wfed + wsta + wloc
                             + mix + pctymle,
                             data = data_crmrte)
se.all_in_model_log_log = sqrt(diag(vcovHC(all_in_model_log_log)))
coeftest(all_in_model_log_log, vcov = vcovHC)

# Model residuals look somwewhat normally distributed with some left skew
hist(all_in_model_log_log$residuals) 

# plot(all_in_model_log_log)
#The residuals vs Fitted plot for this model tells us that 
#the log transformation has significantly reduced the skew in the residuals.
#The clustering and exogenity violations have also been ameliorated 
#but there are more outliers created from the log transform 
#of the dependent variables increasing model error and reducing adj R-squared.



stargazer(all_in_model, all_in_model_log_level, 
          all_in_model_log_log,
          type = "text", omit.stat = "f",
          se = list(se.all_in_model, se.all_in_model_log_level,
                    se.all_in_model_log_log),
          star.cutoffs = c(0.05, 0.01, 0.001))

#Looking at the residual plots and checking for OLS assumption violations,
#we feel the log-level model is the best specified model
#within our initial class of models. This model also strikes the best balance
#between explanatory power and understandability of the variable. 

# We will use the log-level class of models for further refining the 
#specification over the course of the model selection process in this study.

```

The three models above are:\
1. The level-level model\
2. The log-level model\
3. The log-log model\

The log transformed dependent variable shows the best specification and most conformity to OLS assumptions within the 3 models. The log transformed deterrent variables ( log-log model) explain less variation in log_crmrte as compared the log-level model.\

The adjusted r-squared is lower (81.2% vs 73.2%) and the variables with statistical significance (prbarr/log_prbarr, prbconv/log_prbconv) both have higher t-stats using the untransformed variables. Therefore, we will use the log-level model as a base model moving forward.


# Model 1: Simple Model

In order to create a simple model we decided to build using a bottom up approach. We started with a correlation matrix. 

```{r warning=FALSE, fig.align="center"}

drops <- c("county","year", "crmrte", "log_crmrte")
cm = cor(data_crmrte[,!(names(data_crmrte) %in% drops)], data_crmrte$log_crmrte) #Corr Matrix as % for reading clarity

corrdf = data.frame(cm)
corrdf = corrdf[order(-corrdf$cm),,drop = FALSE]
corrdf$pos = (corrdf$cm) + (ifelse(corrdf$cm>0, 0.05, -0.05))
ggplot(data = corrdf,
       aes(y = cm, x=reorder(rownames(corrdf), cm), fill = cm > 0, width=0.9)) + 
       geom_bar(stat = "identity", width=0.9, position = position_dodge(1.4)) + 
       geom_text(aes(label=round(cm,2), y=pos), position=position_dodge(width=0.9)) + 
       coord_flip() + 
       theme(legend.position = "none", panel.grid.minor = element_blank(), panel.grid.major =element_line(size = 1, linetype = 'solid',
                                colour = "#eeeeee"), panel.background = element_blank()) + ylab("Correlation with log(crmrte)") + xlab("Variable")

```

In the above correlation matrix, focusing on the correlations between the independent variable log_crmrte and all other variables in the matrix, density has the highest correlation. This variable makes intuitive sense. As a single variable it might encompass a lot of other factors. More opportunities exist for crime to occur in urban areas (especially when it is unknown what times of crimes are represented (ex:  criminal/fraud/etc)). Below is the simple regression.

```{r fig.align="center"}
simple_regression_model <- lm(log_crmrte ~ density, data = data_crmrte)
se.simple_regression_model = sqrt(diag(vcovHC(simple_regression_model)))
coeftest(simple_regression_model, vcov = vcovHC)

 # AIC is higher than expected for a single variable model
AIC(simple_regression_model)
#The model residuals look normally distributed
hist(simple_regression_model$residuals)
stargazer(simple_regression_model,
          type = "text", omit.stat = "f",
          se = list(se.simple_regression_model),
          star.cutoffs = c(0.05, 0.01, 0.001))

```

Using r-squared, the variable density explains 39.9% of the variation in the log of crime rate . As density increases by 1 unit (as the county population divided by the county land area increases by 1 unit) crime increases by 0.22%. 


# Model 2: Kitchen Sink Model
Still, we can do better in predicting the log crime rate than simply using one variable. We now examine a "kitchen sink" model. This model includes all of the variables in the data set except county (which has too many values to be a useful indicator variable) and year, which is a constant (1987). Below are the results.



```{r}
all_in_model_log_level <- lm(log_crmrte ~ prbarr + prbconv + prbpris 
                             + avgsen + polpc + density
                             + taxpc + west + central + urban 
                             + pctmin80 + wcon
                             + wtuc + wtrd + wfir + wser + wmfg 
                             + wfed + wsta + wloc
                             + mix + pctymle,
                             data = data_crmrte)
se.all_in_model_log_level = sqrt(diag(vcovHC(all_in_model_log_level))) #HC White SE
coeftest(all_in_model_log_level, vcov = vcovHC) 
# Low value of AIC for this model specification which shows good parsimony adjusted fit
AIC(all_in_model_log_level)



stargazer(simple_regression_model, all_in_model_log_level, 
          type = "text", omit.stat = "f",
          se = list(se.simple_regression_model, se.all_in_model_log_level),
          star.cutoffs = c(0.05, 0.01, 0.001))
```

Unsuprisingly, the r-squared of the "kitchen sink" model is substantially higher (85.8% vs. 39.9%). More importantly, the adjusted r-squared which accounts for the number of variables in the models, is also higher (81.2% vs 39.2%). Interestingly, density is no longer the variable with the highest statistical significance. The coefficients show the effect after all the other variables have been controlled for (partialled out). In the "kitchen sink" model prbarr and prbconv both have the lowest p-values and highest statistical significance.\

# Model 3: Balanced Model
There is a middle ground between running a simple regression and using the "kitchen sink" model. We took two approaches to building this balanced model. We used a bottom up approach that relied on both the correlation matrix and stepwise regression. We also used a top down approach that started with the "kitchen sink" model and excluded variables. Both methods are discussed below. \

For the bottom up approach, we started with the simple regression which used density as its only factor. Recall that decision was largely based on the correlation matrix. Next we used stepwise regression in examining each additional variable. The results are below.

```{r results=FALSE}
base_forward = lm(log_crmrte ~ density,
                             data = data_crmrte)
forward_step = step(base_forward, scope = formula(all_in_model_log_level), direction = "forward")


```

With the top down approach, we started with model 3 and looked to exclude variables that weren't as predictive. First, we ran hypothesis testing on all five groups, one group at a time.

```{r}
#deterrent
linearHypothesis(all_in_model_log_level, 
                 c("prbarr = 0", "prbconv = 0", "prbpris = 0",
                   "avgsen = 0", "polpc = 0"), 
                 vcov = vcovHC)
#wage
linearHypothesis(all_in_model_log_level, 
                 c("wcon = 0", "wtuc = 0", "wtrd = 0",
                   "wfir = 0", "wser = 0", "wmfg = 0",
                   "wfed = 0", "wsta = 0", "wloc = 0"), 
                 vcov = vcovHC)
#region
linearHypothesis(all_in_model_log_level, 
                 c("west = 0", "central = 0"), 
                 vcov = vcovHC)
#urban
linearHypothesis(all_in_model_log_level, 
                 c("urban = 0"), 
                 vcov = vcovHC)
#demographic
linearHypothesis(all_in_model_log_level, 
                 c("density = 0", "taxpc = 0", "pctmin80 = 0",
                   "mix = 0", "pctymle = 0"), 
                 vcov = vcovHC)


```


The hypothesis tests show that of the five groups the only groups that are jointly significant are the deterrent data and the demographic data. These tests measure whether removing all the variables within a group reduces the r-squared by a statistically signficant amount. If it does, then at least one variable, and perhaps all variables, within the group should be retained in the model. 

We will re-run the models with just the deterrent and demographic data added and compare. 
```{r fig.align="center"}
balanced_model_top_1 <- lm(log_crmrte ~ prbarr + prbconv + prbpris 
                             + avgsen + polpc + density
                             + taxpc + pctmin80 + mix + pctymle,
                             data = data_crmrte)
se.balanced_model_top_1 = sqrt(diag(vcovHC(balanced_model_top_1)))
coeftest(balanced_model_top_1, vcov = vcovHC)
# Residuals seem like they might not be normally distributed
hist(balanced_model_top_1$residuals) 
# Shapiro test confirm it by rejecting normality
shapiro.test(balanced_model_top_1$residuals) 
#AIC for the model is pretty low
AIC(balanced_model_top_1)

#The residuals vs Fitted plot for this model shows curvature 
# hence likely violation of ZCM and Nr error
# Although the model has high adj R-squares, 
#it's likely the model specification is incorrect.

stargazer(all_in_model_log_level, balanced_model_top_1, 
          type = "text", omit.stat = "f",
          se = list(se.all_in_model_log_level, se.balanced_model_top_1),
          star.cutoffs = c(0.05, 0.01, 0.001))


```
Our adjusted r-squared has only fallen from 81.2% to 76.9% but we have dropped 12 variables. This is a much more parisimous model. 

\
Three of the five groups have been eliminated, with only the deterrent and demographic groups remaining. We will use step wise regression to evaluate.\

```{r fig.align="center"}
base_backward = lm(log_crmrte ~ prbarr + prbconv + prbpris 
                             + avgsen + polpc + density
                             + taxpc + pctmin80 + mix + pctymle,
                             data = data_crmrte)


balanced_model_top_3 <- lm(log_crmrte ~ density
                           + polpc + pctmin80
                           + prbarr + prbconv,
                             data = data_crmrte)
se.balanced_model_top_3 = sqrt(diag(vcovHC(balanced_model_top_3)))
# Residuals look normally distributed
hist(balanced_model_top_3$residuals)
# Shapiro test on residuals confirms normality
shapiro.test(balanced_model_top_3$residuals) 
coeftest(balanced_model_top_3, vcov = vcovHC)
# This model also has low AIC proving good parsimony adjusted fit
AIC(balanced_model_top_3, k=2)
stargazer(all_in_model_log_level, balanced_model_top_3, 
          type = "text", omit.stat = "f",
          se = list(se.all_in_model_log_level, se.balanced_model_top_3),
          star.cutoffs = c(0.05, 0.01, 0.001))

```

The difference between the backward and forward model is that the backward model chooses variables for exclusion based on comparing significance while the forward model looks for significance in inclusion. We also used the f-tests (hypothesis tests) to give the backward stepwise regression a head start.\

The backward stepwise regression yielded a more reasonable model so that is the model we are choosing for our balanced model. This model strikes a nice balance between parsimony and explanatory power. The variables included are prbarr, prbconv, polpc, density and pctmin80. Five out of the original twenty four independent variables are included. The adjusted r-squared is only 3% lower (76.9% vs. 81.2%). It includes a blend of actionable items for the campaign in the deterrent data as well as demographic variables that perhaps can focus the campaign's efforts.

# Final Model

$$
log(crmrte) = \beta_0 + \beta_1 \cdot prbarr + \beta_2 \cdot prbconv + \beta_3 \cdot polpc + \beta_4 \cdot density + \beta_5 \cdot pctmin80
$$

$$
\begin{aligned}
\beta_0 &= -3.26\\
\beta_1 &= -2.276\\
\beta_2 &= -0.758\\
\beta_3 &= 193.390\\
\beta_4 &= 0.116\\  
\beta_5 &= 0.012\\  
\end{aligned}
$$

The above equation shows the final model and its coefficients.

- The deterrent variables (prbarr and prbconv) have negative impact on crime rate indicated by the negative coefficients. As the prbarr increases by one unit (.01), predicted crime rate decreases by 2.28%. As prbconv increases by one unit (.01), predicted crime decreases by 0.76%.
- The demographic variable density and pctmin80 have a positive impact on crime rate.  As density increases by one unit (.01), predicted crime increases by 0.12%. As pctmin80 increases by one unit (.01), predicted crime rate increases by 0.01%.
- The demographic variable polpc has an unexpected positive sign and large magnitude. This does not mean that crime rate increase with increase in police per capita. This effect is caused  because of the simultaneity bias between "polpc" and "crm_rate" which causes violation of the OLS assumptions (which is discussed further in the next few sections) and hence leads to the estimated causal effect of crime rate to suffer from omitted variable bias .Polpc is a control variable in our model and it's coefficient is in violation of the condition mean independence requirement and hence does not have a causal interpretation

Here is a view of the coefficients along with predictions at the 25th percentile, median, mean, and 75th percentile. This gives us both predictions and a sense of the distribution.

![Predictions](Predictions.png)


There are a couple of things to note in looking at this table. First, let's look at the interquartile ranges to assess practical significance. At first glance, pctmin80 doesn't appear to be practically significant as a one unit increase only leads to a .01% increase in predicted crime rate. However, a one unit increase is quite small for this variable. polpc has the tightest interquartile range. We view polpc, density, and pctmin80 more as control variables. They are important for making prediction but we don't view them as part of a political campaign. If we look at prbarr and prbconv, we can realistically hope to improve both variables by 10 units (.10). A ten unit increase in prbarr is similar to going from the 25th to 75th percentile of the existing data set. A ten unit increase in the prbconv is similar to going from the median to the mean. In the event we are able to improve both measures by 10%, we predict crime to fall by 26.17%.  


\pagebreak

# 3. An Assessment of the CLM Assumptions

We choose our balanced model for the complete assessment of all 6 classical linear model assumptions.


![MLR 1-4'](CLM Assumptions1.png)

### MLR.1: The model is linear in parameters ( and the error term)

we haven't constrained the error term, so the model can be any joint distribution. Therefore the linear model assumption is not violated


```{r}
balanced_model_top_3 <- lm(log_crmrte ~  density
                           + polpc + pctmin80
                           + prbarr + prbconv,
                             data = data_crmrte)

```


### MLR.2: Random sampling 

First thing to note is that we are dealing with a single cross-section (1987) of a multi-year panel data.

Secondly this is observational data and not experimental so perfect random sampling is hard to achieve.

CORNWELL – TRUMBULL (1994) specifically state they choose panel data because cross – section data were not able to capture the  real  effect of the  crime  rate  on  several  independent regressors.

The authors identify  that the time-series component of the panel data is able to identify  specific  characteristics  of  county  heterogeneity, which is correlated with the criminal justice variables.

In exploring the effects of county specific heterogeneity, counties next to each other may exhibit similar behaviour. While that may be valid for prediction model the standard errors may be understated causing violation of random sampling

While the balanced model achieves high level of statistical significance for the co-efficients, it's important to be mindful of the limitations of the dataset.

```{r}

se.balanced_model_top_3 = sqrt(diag(vcovHC(balanced_model_top_3)))
coeftest(balanced_model_top_3, vcov = vcovHC)

```

### MLR.3: No perfect multicollinearity

Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are highly linearly related. We have perfect multicollinearity if, for example as in the equation above, the correlation between two independent variables is equal to 100% or negative 100%.

We can check for multicollinearity by making a correlation matrix (though there are other complex ways of checking them like Variance Inflation Factor, which are outside the scope of this study). As seen from the correlation matrix below, there is no perfect multicollinearity in the model but we observe some meaningful correlations between (Prbarr, polpc) and (Prbarr,density). Indicating partial multicollinearity in the data. This correlation makes polpc useful as a control variate in the final regression model but the co-efficient does not have causal interpretaion due to omitted variable bias.

These linear realtionships among the X's don't invalidate the MLR parameters but they lower precision and increase the std-errors in the mdoel 
```{r}
balanced_model <- c( "density","polpc", "pctmin80","prbarr","prbconv")
balanced_model_data <- data_crmrte[balanced_model]
round(cor(balanced_model_data)*100,0) # correlations displayed as % for convenience

```
### MLR.4: Zero Conditional Mean / exogeneity

ZCM is best analysed by studying the regression plots of the residuals. Let's start by looking at the regression plots of the balanced model

```{r out.width="50%", fig.show='hold'}
plot(balanced_model_top_3, 1)
plot(balanced_model_top_3, 2)
```

```{r out.width="50%", fig.show='hold'}
plot(balanced_model_top_3, 3)
plot(balanced_model_top_3, 4)
```


__CLM assumptions analysis from plots__

- Plot 1. The residuals vs. fitted plot indicates that the zero conditional mean assumption is NOT perfectly satisfied but the red line is close enough to zero, a big improvement compared to some of the other models we tested. The non-uniform thickness of the residuals-especially in the middle- indicates possible heteroskedasticity. 
- Plot 2. The Q-Q plot shows that the residuals are not perfectly normally distributed, but the log transform of the crime rate improved the positive skew in the data but has introduced some negative skew
- Plot 3. The scale location plot indicated the presence of heteroskedasticity especially in the middle where the thickness of the band varies and outliers such as '50' and '24' are generating large standardized residuals
- Plot 4. The residuals vs leverage plot shows some of the outliers we had discussed earlier ( observations 51, 25, 84) but the most significantly outlier is observation 51 or county 115 ( having high leverage and Cook's distance >1). This outlier significantly affects our model estimate and likely increases model error.

```{r}
round(cor(balanced_model_top_3$residuals, balanced_model_data)*100,5)

```

Finally we check the correlation between the X's and the errors in the model to ensure there is no endogeneity in the model. The zero correlation is a necessary but not sufficient condition for the presence of omitted variable in the regression. There is a more extensive discussion of omitted variable and their implication on model endogeniety in section 5 of the report.

![MLR 5 & 6'](CLM Assumptions2.png)


### MLR.5: Homoskedasticity

Homoskedasticity describes a situation in which the error term has the same variance across all values of the independent variables.

The regression plots indicate the presence of some heteroskedasticity in the errors let's test if they are statistically significant using the Breusch-Pagan test.

- The Breusch-Pagan test below allows us to test for heteroskedasticity under the 

$H_0:  Homeskedasticity$

```{r}
bptest(balanced_model_top_3)
```
From the BP test, surprisingly we find p-value is not statistically significant, therefore we fail to reject $H_0: Homeskedasticity$.

However, we will still choose to be more conservative and use HC consistent std-errors ( Huber-white Std-errors) using coeftest function from the sandwich package in R. This conservative approach we have taken throughout this report in our model selection process in choosing regressors for different models

### MLR.6: Normality of the error term

Often, if the Y variable is skewed, the error terms will be skewed as well. 

We can check the normality using the Q-Q plot to vizualize the distribution of residuals.

We saw in the earlier section that the crime rate has some positive skew, but we were able to reduce the skew by applying log transform to the crime rate.

We can also run a Shapiro - Wilk test for normality of the residuals

$H_0: Normality$ 
```{r}
shapiro.test(balanced_model_top_3$residuals)
```
The p-value is significant, therefore we reject $H_0: Normality$ 

The non-normality of the residuals is statisitically significant for this model.

There is some negative skew from outlier 51 in the transformed variable, however, since we have n>30 under CLT we have OLS estimators are normally distributed.


\pagebreak
# 4. A Regression Table

__The results were displayed in stargazer using HC standard errors as part of model selction__

- This section has been fully covered under section 2 of the report
- We have include statistical F-tests besides the standard t-tests for regression coefficients to check model validity.
- Additionally the practical significance of the model variable chosen have also been discussed in detail
- Below is the summary of the regression models and the AIC & BIC scores which provides a parsimony adjusted measure of fit


```{r}
stargazer(simple_regression_model, all_in_model_log_level, balanced_model_top_1, balanced_model_top_3,
          type = "text", omit.stat = "f",
          se = list(se.simple_regression_model, se.all_in_model_log_level, 
                    se.balanced_model_top_1, se.balanced_model_top_3),
          star.cutoffs = c(0.05, 0.01, 0.001))




```
__Parismony adjusted model performance__

Though AIC and BIC are both Maximum Likelihood estimate driven and penalize free parameters in an effort to combat overfitting, they do so in ways that result in significantly different behavior. Lets look at one commonly presented version of the methods (which results from stipulating normally distributed errors and other well behaving assumptions):

AIC = -2*ln(likelihood) + 2*k,
and
BIC = -2*ln(likelihood) + ln(N)*k,

where:
k = model degrees of freedom ( K=2 is default for OLS)
N = number of observations 

The quick explanation is:\

- AIC is best for prediction as it is asymptotically equivalent to cross-validation.
- BIC is best for explanation as it is allows consistent estimation of the underlying data generating process.

When N is large the two models will produce quite different results. Then the BIC applies a much larger penalty for complex models, and hence will lead to simpler models than AIC for very large N.

So we check both IC for our model and in both cases a lower value implies a better parsimony adjusted outcome.



```{r}
AIC(simple_regression_model)
AIC(all_in_model_log_level)
AIC(balanced_model_top_1)
AIC(balanced_model_top_3)

```

```{r}
BIC(simple_regression_model)
BIC(all_in_model_log_level)
BIC(balanced_model_top_1)
BIC(balanced_model_top_3)

```
\pagebreak
# 5. Omitted Variables

We know that two conditions must hold true for omitted-variable bias to exist in linear regression:
- The omitted variable must be a determinant of the dependent variable (i.e., its true regression coefficient must not be zero); and
- The omitted variable must be correlated with an independent variable specified in the regression (i.e., cov(z,x) must not equal zero).

In our model the presence of omitted variables leads to the error term being correlated with the regressors.
- The presence of omitted-variable bias causes the OLS estimator to be biased and inconsistent.
- The direction of the bias depends on the estimators as well as the covariance between the regressors and the omitted variables.
- A positive covariance of the omitted variable with both a regressor and the dependent variable will lead the OLS estimate of the included regressor's coefficient to be greater than the true value of that coefficient.


We've identified several key omitted variables that we feel most influence the crime rate but are not represented in the data here.


1. Unemployment Rate - Unemployment is a key indicator for crime rate.  We may be able to infer some indication of the frequency of seasonal or part-time work in the construction or service industries from the `wcon` or `wser` variables as they shows an average weekly wage which mght indicate how often workers are employed. However, this estimate is likely not accurate enough to be considered meaningful.
Unemployment rates among Americans from minority groups in many locations in the United States are significantly higher than for Americans who identify as white. We can posit that this may have positive bias on `pctmin80`, the percentage of minority residents in the county, and that it would give it more significance than it may deserve in predicting crime rate.

2. Inflation Rate (Consumer Price Index) - Inflation and crime rates are correlated with a positive relationship and the causal link is from inflation and unemployment to crime. [Link](https://www.researchgate.net/publication/236736987_Will_Inflation_Increase_Crime_Rate_New_Evidence_from_Bounds_and_Modified_Wald_Tests). Inﬂation causes the purchasing power to reduce and cost of living to increase, consequently crime rates rise as the inflation rate rises. Because of the lag between price and wage adjustments, inflation lowers the real income of low-skilled labor, but rewards property criminals due to the rising demand and subsequent high profits in the illegal market. Inflation in the year represented, 1987, would not be sufficient though as the reduction in purchasing power does not happen immediately, it takes time for inﬂation to gradually reduce purchasing power. None of the data provided in the study gives us an indication of the inflation rate in a time period before the study however inflation rate and unemployment may be correlated and may have positive bias on one another. These then also would have a positive bias on the percentage of minority residents in the county in predicting crime rate, biasing `pctmin80` away from zero.

3. Childhood Blood Lead Levels (with 18 year lag offset) - The lead–crime hypothesis is the proposed link between elevated blood lead levels in children and increased rates of crime, delinquency, and recidivism later in life. Studies linking blood lead levels (BLL) in children to crime rate typically seek to quantify the BLL 17-18 years before the examined crime rate. One such study used a unique dataset linking preschool blood lead levels (BLLs), birth, school, and detention data for 120,000 children born 1990-2004 in Rhode Island, to estimate the impact of lead on behavior [Link](https://www.nber.org/papers/w23392.pdf). The `density` varaible would most likely have a positive correlation with BLL as urban and more industrialized areas typically had greater levels of lead poisoning in groundwater and street surfaces due to heavier vehicle traffic and industrial emissions in denser areas. As density had a positive coefficient, we believe that the omitted variable bias is positive away from zero. This may lead ascribing greater significance to `density` in predicting `crmrte`, particularly in the period up until 18 years after the phase out of leaded gas, which 1987 was within.

4. Income Inequality metrics: There are several measures of income inequaity that could be included in the data: [Mean Log Deviation](https://www.census.gov/topics/income-poverty/income-inequality/about/metrics/mld.html) or [Theil Index](https://www.census.gov/topics/income-poverty/income-inequality/about/metrics/theil-index.html) or [Gini Index](https://www.census.gov/topics/income-poverty/income-inequality/about/metrics/gini-index.html) for each of the counties. Income inequality has been shown to have a significant effect on violent crime in particular. One World Bank report states that inequality predicts about half of the variance in murder rates between American states and between countries around the world. [Link](https://siteresources.worldbank.org/DEC/Resources/Crime%26Inequality.pdf) Income inequality measures are often measured as 0 (perfectly equal income distribution) to 1 (perfectly unequal income distribution, or 1 household has all the income). We would thus expect these to have a positive bias, in that an increase in income inequality would lead to an increase in violent crime. A correlation in American cities between density and income inequality has been shown by [Edward Glaseer](https://scholar.harvard.edu/files/glaeser/files/urban_inequality.pdf) leading us to believe that income inequality shows a positive bias on `density` We believe that the omitted variable bias is positive away from zero and that this plays some role in over-estimating the significance of density in predicting crime rate.

**Summary:**

*Unemployment rate* is positively correlated with crime rate and positively correlated with percentage of minorities in each county. The coefficient on `pctmin80` is 0.012099, therefore the omitted variable bias is positive away from zero.

*Inflation rate* is also positively correlated with crime rate and positively correlated with percentage of minorities in each county. The coefficient on `pctmin80` is 0.012099, therefore the omitted variable bias is positive away from zero.

*Childhood BLL* is positively correlated with crime rate and positively correlated with density. The coefficient on `density` is 0.115702, so we surmise that the bias is positive away from zero.

*Income inequality* is positively correlated with crime rate and positively correlated with density. The coefficient on `density` is 0.012099 and we surmise that as density rises that income inequality rises as well, so the bias is positive away from zero.


\pagebreak
# 6. A Conclusion

Using the 1987 dataset, we were able to identify the key demographic and deterrent variables that affect crime. Our final model shows that arrests (prbarr), convictions (prbconv), and police presence (polpc) are deterrents that affect crime rates. As indicated by the negative coefficients, an increase in arrests and convictions predict a decrease in crime rate, suggesting that these variables are key deterrents in reducing crime. While the probability of arrest and convictions are statistically significant, they also have practical significance. Our study shows improving the measures of arrest and convincitions by 10% could potentially lead a reduction in crime rate by 25%. This is a number that can resonate with voters and we recommend that this headlining statistic lead the campaign. 

While the coefficeint for police presence predicts an increase in crime rates, it should be noted that this does not indicate that additional police cause an increase in crime. The results suggest that with additional police there will be an initial increase in apprehension of criminals, resulting in an increase in police reports. The actual reduction in committed crimes due to a larger police presence may see a lag effect which could potentially be corrected by the use of Fixed Effects regression model. In addition, police presence and crime rate affect each other and causes simultaneity bias in the model, which states that the dependent and indepedent variable influence each other at the same time. Cornwell and Trunbull(1994) also raise this issue in their paper and they employ 2 SLS model to adjust for the effects of simultaneity. While this issue is worth reviewing further, it is outside the scope of this report. 

The demographic categories of  density, minorities (pctmin80), and offense (mix) are statistically significant variables of crime rate. While the results show a strong statistical relationship with crime rates and percentage of minorities, this result does not definitively denote a causal affect that an increase in minorities predicts an increase in crime. The complex nature of race relations should be considered. The relationship between minorties and crime could be a result of racial bias within the police force and criminal justice system, leading to a disproportionate number of minorities being arrested and convicted for crimes. 

While the results of this report have shown that the economic data does not hold statistical significance in our analysis, that is not an indication that economic factors are not key determinants on crime. The wage statistics provided in the dataset, while important, do not address issues of poverty and inequality which are traditional drivers of crime. As described in the report, omitted variables such as unemployment rate, inflation rate, and income inequality are key economic factors that have a significant affect on crime and should be examined further.     

Based on the results of our study, we propose a political strategy that will focus on deterrents and demographic factors to address crime: 

* The first recommendation is to increase the number of arrests by focusing on providing the police force with the proper funding, training and resources. As shown in the results, the prbarr is a practically significant deterrent of crime showing that a .01 increase in the variable would predict a 2.25% decrease in crime rate. 

* The second recommendation is to increase the number of convictions by providing additional resources to the criminal justice system. However, it is recommended that punishment for convictions should not be prison sentences unless necessary and justified. Our study suggests that prison sentences are not a significant deterrent of crime, while also having adverse effects of overcrowding of the prison system and burdening taxpayers.

* The third recommendation is to focus on developing local economies and increasing job opportunities in areas of high unemployment. The results of this study show a significant associative relationship between minorities and crime rate that should be addressed. While the dataset is limited in determining causal reasons for this issue, our team has identified employment rate as an omitted variable that disproportionately impacts minorities. We recommend providing resources that can stimulate local economies and boost the employment rate.
